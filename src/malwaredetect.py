
# coding: utf-8

# In[ ]:

import pandas
import datetime as dt

from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *
from bigdl.dataset.transformer import *
from bigdl.dataset import mnist
from matplotlib.pyplot import imshow
import matplotlib.pyplot as plt
from pyspark import SparkContext
from bigdl.util import common
from bigdl.dataset import mnist
sc=SparkContext.getOrCreate(conf=create_spark_conf().setMaster("local[4]").set("spark.driver.memory","14g"))

#constants
batch_size=256
learning_rate=0.4
noofepochs=1


# In[ ]:


def get_malware(sc, malware_path):
    # target is start from 0,
    (train_images, train_labels) = mnist.read_data_sets(malware_path, "train")
    (test_images, test_labels) = mnist.read_data_sets(malware_path, "test")
    training_mean = np.mean(train_images)
    training_std = np.std(train_images)
    rdd_train_images = sc.parallelize(train_images)
    rdd_train_labels = sc.parallelize(train_labels)
    rdd_test_images = sc.parallelize(test_images)
    rdd_test_labels = sc.parallelize(test_labels)

    rdd_train_sample = rdd_train_images.zip(rdd_train_labels).map(lambda (features, label):
                                        common.Sample.from_ndarray(
                                        (features - training_mean) / training_std,
                                        label + 1))
    rdd_test_sample = rdd_test_images.zip(rdd_test_labels).map(lambda (features, label):
                                        common.Sample.from_ndarray(
                                        (features - training_mean) / training_std,
                                        label + 1))
    return (rdd_train_sample, rdd_test_sample)


# In[ ]:

mnist_path = "datasets/mnist"
(train_data, test_data) = get_malware(sc, mnist_path)

#print train_data.count()
#print test_data.count()


# In[6]:

def build_model(class_num):
            model = Sequential()
            model.add(Reshape([1, 32, 32]))
            #(<input plane>,<output plane>,<kwidth>,<kheight>)
            model.add(SpatialConvolution(1, 64, 3, 3).set_name('conv1'))
            model.add(ReLU())
            #(<stride>,<stride>,<kwidht>,<kheight>)
            model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool1'))
            model.add(ReLU())
            model.add(SpatialConvolution(64, 128, 3, 3).set_name('conv2'))
            model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool2'))
            model.add(ReLU())
            model.add(SpatialConvolution(128, 256, 2, 2).set_name('conv3'))
            model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool3'))
            model.add(ReLU())
            model.add(SpatialConvolution(256, 512, 2, 2).set_name('conv4'))
            model.add(Reshape([512]))
            model.add(Linear(512, 100).set_name('fc1'))
            model.add(ReLU6())
            model.add(Linear(100, class_num).set_name('output'))
            model.add(LogSoftMax())
            return model


# In[ ]:

dsp_black_model=build_model(9)
optimizer = Optimizer(
    model=dsp_black_model,
    training_rdd=train_data,
    criterion=ClassNLLCriterion(),
    optim_method=SGD(learningrate=learning_rate, learningrate_decay=0.0002),
    end_trigger=MaxEpoch(noofepochs),
    batch_size=batch_size)

# Set the validation logic
optimizer.set_validation(
    batch_size=256,
    val_rdd=test_data,
    trigger=EveryEpoch(),
    val_method=[Top1Accuracy()]
)


# In[ ]:

app_name='dspblack-'+dt.datetime.now().strftime("%Y%m%d-%H%M%S")
train_summary = TrainSummary(log_dir='/tmp/bigdl_summaries',
                                     app_name=app_name)
train_summary.set_summary_trigger("Parameters", SeveralIteration(50))
val_summary = ValidationSummary(log_dir='/tmp/bigdl_summaries',
                                        app_name=app_name)
optimizer.set_train_summary(train_summary)
optimizer.set_val_summary(val_summary)
print "saving logs to ",app_name


# In[ ]:

print "Training.....!"
trained_model =  optimizer.optimize()
print "End of Traininng... :) "


# In[ ]:

print "Predictions.."
predictions = trained_model.predict(test_data)


# In[ ]:

predictions.collect()

