
# coding: utf-8

# In[12]:

import pandas
import datetime as dt


from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *
from bigdl.dataset.transformer import *
#from bigdl.dataset import mnist
from matplotlib.pyplot import imshow
import matplotlib.pyplot as plt
from pyspark import SparkContext
from bigdl.util import common
from bigdl.dataset import mnist
sc=SparkContext.getOrCreate(conf=create_spark_conf().setMaster("local[4]").set("spark.driver.memory","14g"))


#hyper-Parameters
batch_size=256
learning_rate=0.4
noofepochs=1

init_engine()

import os
from PIL import Image
import numpy as np
def read_data_sets(main_path,dataset):
    #pre-proccess images 
    filelist=os.listdir(main_path+dataset)
    imagelist_pre = np.array([np.array(Image.open(main_path+dataset+fname)) for fname in filelist])
    imagelist = imagelist_pre[..., None]
    #pre-proccess Labels to match the images
    label_file = open("32by32/Train_label.txt", "r")
    labels = label_file.read().split("\n")
    correctlabels=labels
    for i in range(len(filelist)):
        filelist[i]=filelist[i].replace("X_train","")
        filelist[i]=filelist[i].replace("X_test","")
        filelist[i]=int(filelist[i].replace(".png",""))
        correctlabels[i]=labels[filelist[i]]
    
    return (imagelist,correctlabels)



#-----------test------------------
#(x,y)=read_data_sets("32by32/","train/")
#y=read_data_sets("32by32/","test/")

#x=np.array([np.array(Image.open("32by32/train/X_train0.png"))])
#print (x.shape)
#print (y.shape)
  


def get_malware(sc, malware_path):
    # target is start from 0,
    
    (train_images,train_labels) = read_data_sets(malware_path, "train/")
    (test_images,test_labels) = read_data_sets(malware_path, "test/")
    
    training_mean = np.mean(train_images)
    training_std = np.std(train_images)
    rdd_train_images = sc.parallelize(train_images)
    rdd_train_labels = sc.parallelize(train_labels)
    rdd_test_images = sc.parallelize(test_images)
    rdd_test_labels = sc.parallelize(test_labels)

    rdd_train_sample = rdd_train_images.zip(rdd_train_labels).map(lambda (features, label):
                                        common.Sample.from_ndarray(
                                        (features - training_mean) / training_std,
                                        label + 1))
    rdd_test_sample = rdd_test_images.zip(rdd_test_labels).map(lambda (features, label):
                                        common.Sample.from_ndarray(
                                        (features - training_mean) / training_std,
                                        label + 1))
    return (rdd_train_sample, rdd_test_sample)



dataset_path = "32by32/"
(train_data, test_data) = get_malware(sc, dataset_path)

#print train_data.count()
#print test_data.count()



def build_model(class_num):
            model = Sequential()
            model.add(Reshape([1, 32, 32]))
            #(<input plane>,<output plane>,<kwidth>,<kheight>)
            model.add(SpatialConvolution(1, 64, 3, 3).set_name('conv1'))
            model.add(ReLU())
            #(<stride>,<stride>,<kwidht>,<kheight>)
            model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool1'))
            model.add(ReLU())
            model.add(SpatialConvolution(64, 128, 3, 3).set_name('conv2'))
            model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool2'))
            model.add(ReLU())
            model.add(SpatialConvolution(128, 256, 2, 2).set_name('conv3'))
            model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool3'))
            model.add(ReLU())
            model.add(SpatialConvolution(256, 512, 2, 2).set_name('conv4'))
            model.add(Reshape([512]))
            model.add(Linear(512, 100).set_name('fc1'))
            model.add(ReLU6())
            model.add(Linear(100, class_num).set_name('output'))
            model.add(LogSoftMax())
            return model




dsp_black_model=build_model(9)
optimizer = Optimizer(
    model=dsp_black_model,
    training_rdd=train_data,
    criterion=ClassNLLCriterion(),
    optim_method=SGD(learningrate=learning_rate, learningrate_decay=0.0002),
    end_trigger=MaxEpoch(noofepochs),
    batch_size=batch_size)

# Set the validation logic
optimizer.set_validation(
    batch_size=256,
    val_rdd=test_data,
    trigger=EveryEpoch(),
    val_method=[Top1Accuracy()]
)




app_name='dspblack-'+dt.datetime.now().strftime("%Y%m%d-%H%M%S")
train_summary = TrainSummary(log_dir='/tmp/bigdl_summaries',
                                     app_name=app_name)
train_summary.set_summary_trigger("Parameters", SeveralIteration(50))
val_summary = ValidationSummary(log_dir='/tmp/bigdl_summaries',
                                        app_name=app_name)
optimizer.set_train_summary(train_summary)
optimizer.set_val_summary(val_summary)
print "saving logs to ",app_name


print "Training.....!"
trained_model =  optimizer.optimize()
print "End of Traininng... :) "

print "Predictions.."
predictions = trained_model.predict(test_data)


predictions.collect()

